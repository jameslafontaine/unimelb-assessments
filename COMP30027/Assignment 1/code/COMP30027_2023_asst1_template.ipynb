{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Assignment 1: Music genre classification with naive Bayes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     1079860\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RUN ALL CELLS IN ORDER FROM TOP TO BOTTOM AND THE CODE SHOULD WORK AS INTENDED ##\n",
    "\n",
    "\n",
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "import sklearn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "           \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihood parameters from the training data and using them to build a naive Bayes model\n",
    "def train(df):\n",
    "\tnaive_bayes = {}\n",
    "\tnaive_bayes['priors'] = calc_prior(df)\n",
    "\tnaive_bayes['likelihood_params'] = calc_likelihood_params(df)\n",
    "\treturn naive_bayes\n",
    "\n",
    "\n",
    "\n",
    "# Calculates the prior probabilities for each class label\n",
    "def calc_prior(df):\n",
    "\tprior_prob = {}\n",
    "\tlabels = df.values[:, -1] # all rows, last column\n",
    "\tn = len(labels)\n",
    "\tunique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\t#print(unique_labels)\n",
    "\n",
    "\tfor i in range(len(unique_labels)):\n",
    "\t\tprior_prob[unique_labels[i]] = (counts[i] / n)\n",
    "\n",
    "\treturn prior_prob\n",
    "\n",
    "\n",
    "# Calculates the mean and variance of each class x label combination which define a gaussian distribution for them\n",
    "def calc_likelihood_params(df):\n",
    "\t# Create an empty dictionary to store the likelihood parameters\n",
    "\tlikelihood_params = {}\n",
    "\tunique_labels = np.unique(df.values[:, -1])\n",
    "\tfeatures_list = df.columns[1:-1]\n",
    "\t\n",
    "\tfor feature in features_list:\n",
    "\n",
    "\t\t# Create a nested dictionary for each feature\n",
    "\t\tlikelihood_params[feature] = {}\n",
    "\n",
    "\t\tfor label in unique_labels:\n",
    "\t\t\t# Create a nested dictionary for each label\n",
    "\t\t\tlikelihood_params[feature][label] = {}\n",
    "\n",
    "\t\t\t# Create a list of feature values for this feature from the instances which match the current label\n",
    "\t\t\tfeature_values = ((df[df['label'] == label])[feature])\n",
    "\t\t\tmean = 0\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the mean for this feature x label combination\n",
    "\t\t\tfor value in feature_values:\n",
    "\t\t\t\tmean += value / len(feature_values)\n",
    "\t\t\t#print(f\"Mean of {feature},{label} is {mean:.4f}\")\n",
    "\n",
    "\t\t\tsum_sqr_deviations = 0\n",
    "\t\t\t# Calculate the sum of squared deviations for this feature x label combination so we can calculate the variance\n",
    "\t\t\tfor value in feature_values:\n",
    "\t\t\t\tsum_sqr_deviations += (value - mean)**2\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the variance for this feature x label combination\n",
    "\t\t\tvar = sum_sqr_deviations / (len(feature_values) - 1)\n",
    "\t\t\t#print(f\"Variance of {feature},{label} is {var:.4f}\")\n",
    "\n",
    "\t\t\t# Store the parameters in the dictionary\n",
    "\t\t\tlikelihood_params[feature][label]['mean'] = mean\n",
    "\t\t\tlikelihood_params[feature][label]['var'] = var\n",
    "\n",
    "\treturn likelihood_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "# This function should predict classes for new items in a test dataset\n",
    "def predict(test_df, prior_prob, likelihood_params):\n",
    "\targmax_labels = []\n",
    "\n",
    "\tfor _, test_inst in test_df.iterrows():\n",
    "\t\t# Calculate posterior probabilities for each class for this test instance\n",
    "\t\tposterior_probs = calc_posterior(test_inst, prior_prob, likelihood_params)\n",
    "\t\t\n",
    "\t\t# Find argmax for this test instance\n",
    "\t\t#print(\"Now finding argmax for this instance\")\n",
    "\t\tfor post_prob in posterior_probs:\n",
    "\t\t\tmax_prob = float(-inf)\n",
    "\t\t\tmax_label = None\n",
    "\t\t\tfor label in post_prob:\n",
    "\t\t\t\t#print(f\"Current label: {label}\")\n",
    "\t\t\t\tif label in prior_prob:\n",
    "\t\t\t\t\tprob = post_prob[label]\n",
    "\t\t\t\t\t#print(f\"Current log-prob: {prob}\")\n",
    "\t\t\t\t\tif prob > max_prob:\n",
    "\t\t\t\t\t\tmax_prob = prob\n",
    "\t\t\t\t\t\tmax_label = label\n",
    "\t\t\tif max_label is not None:\n",
    "\t\t\t\targmax_labels.append(max_label)\n",
    "\t\t\telse:\n",
    "\t\t\t\targmax_labels.append('none')\n",
    "\treturn argmax_labels   \n",
    "\n",
    "\n",
    "import math\n",
    "log = math.log\n",
    "\n",
    "# Computes the gaussian pdf for a given x value, mean and variance\n",
    "def pdfnorm(x, mean, var):\n",
    "\tsqrt = math.sqrt\n",
    "\tPI = math.pi\n",
    "\te = math.e\n",
    "\t\n",
    "\treturn (1/(sqrt(2*PI*var))) * e ** -(((x-mean)**2)/(2*var))\n",
    "\n",
    "\n",
    "# Calculates the log of the posterior probabilities for each class given a test instance\n",
    "def calc_posterior(test_inst, prior_prob, likelihood_params):\n",
    "\tposterior_probs = []\n",
    "\n",
    "\tpost_probs = {}\n",
    "\tfor label in prior_prob:\n",
    "\t\tpost_probs[label] = log(prior_prob[label])\n",
    "\t\t#print(f\"Current label = {label}\")\n",
    "\t\tfor feature in likelihood_params:\n",
    "\t\t\tpost_prob = pdfnorm(test_inst[feature], likelihood_params[feature][label]['mean'], likelihood_params[feature][label]['var'])\n",
    "\t\t\t#print(f\"Current value = {test_inst[feature]}\")\n",
    "\t\t\t#print(f\"Current feature = {feature}\")\n",
    "\t\t\t#print(f\"Current mean = {likelihood_params[feature][label]['mean']}\")\n",
    "\t\t\t#print(f\"Current variance = {likelihood_params[feature][label]['var']}\")\n",
    "\t\t\t#print(f\"pdfnorm({feature}, {likelihood_params[feature][label]['mean']}, {likelihood_params[feature][label]['var']}) = {(pdfnorm(test_instance[feature], likelihood_params[feature][label]['mean'], likelihood_params[feature][label]['var']))}\")\n",
    "\t\t\t#print(f\"log(pdfnorm({feature}, {likelihood_params[feature][label]['mean']}, {likelihood_params[feature][label]['var']})) = {log(pdfnorm(test_instance[feature], likelihood_params[feature][label]['mean'], likelihood_params[feature][label]['var']))}\")\n",
    "\n",
    "\t\t\t# cover the case where posterior probability is 0 to avoid domain error\n",
    "\t\t\tif post_prob != 0:\n",
    "\t\t\t\tpost_probs[label] += log(post_prob)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpost_probs[label] = float(-inf)\n",
    "\t\t\t\tbreak\n",
    "\tposterior_probs.append(post_probs)\n",
    "\n",
    "\treturn posterior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "\n",
    "# This function should evaluate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(naive_bayes, test_df):\n",
    "    evaluation = {}\n",
    "    ground_truths = test_df['label']\n",
    "    predictions = predict(test_df, naive_bayes['priors'], naive_bayes['likelihood_params'])\n",
    "\n",
    "    #print(ground_truths)\n",
    "    #print(predictions)\n",
    "\n",
    "    # Store exhaustive evaluation metrics within a dictionary so they can be accessed as needed\n",
    "    evaluation['report'] = skm.classification_report(ground_truths, predictions, output_dict=True)\n",
    "    evaluation['accuracy'] = skm.accuracy_score(ground_truths, predictions)\n",
    "    evaluation['precision'] = {}\n",
    "    evaluation['precision']['macro'] = skm.precision_score(ground_truths, predictions, average=\"macro\")\n",
    "    evaluation['precision']['micro'] = skm.precision_score(ground_truths, predictions, average=\"micro\")\n",
    "    evaluation['precision']['weighted'] = skm.precision_score(ground_truths, predictions, average=\"weighted\")\n",
    "    evaluation['recall'] = {}\n",
    "    evaluation['recall']['macro'] = skm.recall_score(ground_truths, predictions, average=\"macro\")\n",
    "    evaluation['recall']['micro'] = skm.recall_score(ground_truths, predictions, average=\"micro\")\n",
    "    evaluation['recall']['weighted'] = skm.recall_score(ground_truths, predictions, average=\"weighted\")\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "#evaluation = evaluate(train(preprocess(\"gztan_train.csv\")), preprocess(\"gztan_test.csv\"))\n",
    "#print(evaluation['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Pop vs. classical music classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Compute and report the accuracy, precision, and recall of your model (treat \"classical\" as the \"positive\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "evaluation = evaluate(train(preprocess(\"pop_vs_classical_train.csv\")), preprocess(\"pop_vs_classical_test.csv\"))\n",
    "clasf_report = pd.DataFrame.from_dict(evaluation['report']).round(4)\n",
    "display(clasf_report)\n",
    "#pd.DataFrame.from_dict(evaluation['report'])\n",
    "\n",
    "#classical precision = 95.2381%  => a few false positives\n",
    "#classical recall = 100% => 0 false negatives => all true positives were captured\n",
    "#accuracy = 97.6744%\n",
    "\n",
    "#clasf_report.to_csv('task1q1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "For each of the features X below, plot the probability density functions P(X|Class = pop) and P(X|Class = classical). If you had to classify pop vs. classical music using just one of these three features, which feature would you use and why? Refer to your plots to support your answer.\n",
    "- spectral centroid mean\n",
    "- harmony mean\n",
    "- tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = preprocess(\"pop_vs_classical_train.csv\")\n",
    "likelihood_params = calc_likelihood_params(train_df)\n",
    "\n",
    "feature_list = ['spectral_centroid_mean', 'harmony_mean', 'tempo']\n",
    "\n",
    "labels = ['pop', 'classical']\n",
    "\n",
    "\n",
    "# Create a list of x and y values to plot the pdf with using the range of the feature values\n",
    "x_vals = {}\n",
    "y_vals = {}\n",
    "for label in labels:\n",
    "    x_vals[label] = {}\n",
    "    y_vals[label] = {}\n",
    "    for feature in feature_list:\n",
    "        x_min = min(train_df[feature])\n",
    "        x_max = max(train_df[feature])\n",
    "        x_inc = abs((x_min + x_max) / 100)\n",
    "        feature_values = (train_df[train_df['label'] == label])[feature]\n",
    "        x_vals[label][feature] = []\n",
    "        y_vals[label][feature] = []\n",
    "        i = x_min\n",
    "        while (i < x_max):\n",
    "            x_vals[label][feature].append(i)\n",
    "            i += x_inc\n",
    "        #x_vals[label][feature] = sorted(x_vals[label][feature])\n",
    "        #print(x_vals[label][feature])\n",
    "        y_vals[label][feature] = [pdfnorm(x, likelihood_params[feature][label]['mean'], likelihood_params[feature][label]['var']) for x in x_vals[label][feature]]\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "for feature in feature_list:\n",
    "    plt.plot(x_vals['pop'][feature], y_vals['pop'][feature], label = \"P(X|Class = Pop)\")\n",
    "    plt.plot(x_vals['classical'][feature], y_vals['classical'][feature], label = \"P(X|Class = Classical)\")\n",
    "    plt.xlabel('feature value')\n",
    "    plt.ylabel('pdf(x)')\n",
    "    plt.title(f\"{feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# If we had to classify pop or classical music using just one of these features then using the spectral centroid mean would be the best choice. This feature has the least amount of \n",
    "# overlap present between the probability density functions for both classes and therefore we can predict the class\n",
    "# based on this attribute with a high degree of certainty. If the spectral centroid mean for an instance is below approximately 2000 then there is a larger probability that the instance \n",
    "# is an example of classical music and simulatenously a small probability that the instance is pop music, and vice versa if the spectral centroid mean is above 2000. Due to the lack of overlap between the PDs, \n",
    "# there is a very small chance of an error when a prediction is made purely using the probability density in relation to this single feature.\n",
    "#  If the other 2 features were to be used to make predictions, predicting the class label\n",
    "# based on which class has the higher probability density would likely result in more errors in our predictions due to the greater overlap between the probability densities for those attributes. (could provide example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. 10-way music genre classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare the performance of the full model to a 0R baseline and a one-attribute baseline. The one-attribute baseline should be the best possible naive Bayes model which uses only a prior and a single attribute. In your write-up, explain how you implemented the 0R and one-attribute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Train and test your model with a range of training set sizes by setting up your own train/test splits. With each split, use cross-fold validation so you can report the performance on the entire dataset (1000 items). You may use built-in functions to set up cross-validation splits. In your write-up, evaluate how model performance changes with training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_df = pd.concat([preprocess(\"gztan_train.csv\"), preprocess(\"gztan_test.csv\")])\n",
    "\n",
    "#complete_df.head()\n",
    "#complete_df.tail()\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "k_folds = [2,5,10]\n",
    "evaluations = {}\n",
    "concat_reports = {}\n",
    "averaged_reports = {}\n",
    "\n",
    "for k in k_folds:\n",
    "\t#print(k)\n",
    "\tevaluations[k] = []\n",
    "\tkf = StratifiedKFold(n_splits = k)#, shuffle = True, random_state = 2)\n",
    "\t\n",
    "\t# evaluate each training/test split combination generated with k folds\n",
    "\tfor i, (train_index, test_index) in enumerate(kf.split(complete_df.values[:, 1:-1], complete_df.values[:,-1])):\n",
    "\t\ttrain_df = complete_df.iloc[train_index]\n",
    "\t\ttest_df =  complete_df.iloc[test_index]\n",
    "\n",
    "\t\tevaluations[k].append(evaluate(train(train_df), test_df))\n",
    "\treports = []\n",
    "\tfor eval in evaluations[k]:\n",
    "\t\treports.append(pd.DataFrame.from_dict(eval['report']))\n",
    "\t\n",
    "\tconcat_reports[k] = pd.concat(reports)\n",
    "\t#display(concat_reports[k].index)\t\n",
    "\t#display(concat_reports[k].columns)\t\n",
    "\t#display(concat_reports[k])\n",
    "\t# average out evaluation metrics across the different training / test splits for the current k-folds\n",
    "\taveraged_reports[k] = ((concat_reports[k]).groupby(by=concat_reports[k].index, axis=0).mean())\n",
    "\tdisplay(averaged_reports[k].round(3))\n",
    "\t#(averaged_reports[k].round(3)).to_csv(f\"averaged-report-{k}-folds.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement a kernel density estimate (KDE) naive Bayes model and compare its performance to your Gaussian naive Bayes model. You may use built-in functions and automatic (\"rule of thumb\") bandwidth selectors to compute the KDE probabilities, but you should implement the naive Bayes logic yourself. You should give the parameters of the KDE implementation (namely, what bandwidth(s) you used and how they were chosen) in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Modify your naive Bayes model to handle missing attributes in the test data. Recall from lecture that you can handle missing attributes at test by skipping the missing attributes and computing the posterior probability from the non-missing attributes. Randomly delete some attributes from the provided test set to test how robust your model is to missing data. In your write-up, evaluate how your model's performance changes as the amount of missing data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
